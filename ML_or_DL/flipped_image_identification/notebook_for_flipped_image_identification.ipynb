{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KaziNoshin_Coding Challenge for Fatima Fellowship",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBpjBBZc6IvA"
      },
      "source": [
        "# Fatima Fellowship Quick Coding Challenge (Pick 1)\n",
        "\n",
        "Thank you for applying to the Fatima Fellowship. To help us select the Fellows and assess your ability to do machine learning research, we are asking that you complete a short coding challenge. Please pick **1 of these 5** coding challenges, whichever is most aligned with your interests. \n",
        "\n",
        "**Due date: 1 week**\n",
        "\n",
        "**How to submit**: Please make a copy of this colab notebook, add your code and results, and submit your colab notebook to the submission link below. If you have never used a colab notebook, [check out this video](https://www.youtube.com/watch?v=i-HnvsehuSw).\n",
        "\n",
        "**Submission link**: https://airtable.com/shrXy3QKSsO2yALd3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "braBzmRpMe7_"
      },
      "source": [
        "# 1. Deep Learning for Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IWw-NZf5WfF"
      },
      "source": [
        "**Upside down detector**: Train a model to detect if images are upside down\n",
        "\n",
        "* Pick a dataset of natural images (we suggest looking at datasets on the [Hugging Face Hub](https://huggingface.co/datasets?task_categories=task_categories:image-classification&sort=downloads))\n",
        "* Synthetically turn some of images upside down. Create a training and test set.\n",
        "* Build a neural network (using Tensorflow, PyTorch, or any framework you like)\n",
        "* Train it to classify image orientation until a reasonable accuracy is reached\n",
        "* [Upload the the model to the Hugging Face Hub](https://huggingface.co/docs/hub/adding-a-model), and add a link to your model below.\n",
        "* Look at some of the images that were classified incorrectly. Please explain what you might do to improve your model's performance on these images in the future (you do not need to impelement these suggestions)\n",
        "\n",
        "**Submission instructions**: Please write your code below and include some examples of images that were classified"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pick a dataset of natural images (we suggest looking at datasets on the Hugging Face Hub)"
      ],
      "metadata": {
        "id": "KHgqCWgbsJMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mnist\")"
      ],
      "metadata": {
        "id": "K2GJaYBpw91T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "NKBeJKMns0ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Synthetically turn some of images upside down. Create a training and test set."
      ],
      "metadata": {
        "id": "UbW4Y0bcsrJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "p-se302f3EYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to make train and test dataset\n",
        "\n",
        "dim = np.array(dataset['train'][0][\"image\"]).shape[0]\n",
        "channel_count = 1\n",
        "img_str = \"image\"\n",
        "\n",
        "def convert_img_to_tensor(image):\n",
        "  transform = transforms.Compose([transforms.PILToTensor()])\n",
        "\n",
        "  img_tensor = transform(image).reshape(1,dim,dim,channel_count)\n",
        "\n",
        "  img_tensor = np.pad(img_tensor, ((0,0), (2,2), (2,2), (0,0)))\n",
        "\n",
        "  # print(img_tensor.shape)\n",
        "  return img_tensor\n",
        "\n",
        "def get_tensor_array(mydf):\n",
        "  arr_orig = np.array([])\n",
        "\n",
        "  for i in range(len(mydf)):\n",
        "    image = mydf[img_str][i]\n",
        "    img_tensor = convert_img_to_tensor(image)\n",
        "    if i == 0:\n",
        "      arr_orig = np.array(img_tensor)\n",
        "    else:\n",
        "      arr_orig = np.concatenate((arr_orig, img_tensor))\n",
        "\n",
        "  return arr_orig\n",
        "\n",
        "\n",
        "def do_preprocessing(mydf, data_count, if_train):\n",
        "  mydf_X_orig = mydf.copy()\n",
        "\n",
        "  # if if_train == 1:\n",
        "  #   # keeping the ratio of the numbers same in the training dataset\n",
        "  #   y = mydf_X_orig[\"label\"]\n",
        "  #   X = mydf_X_orig.drop('label', axis=1)\n",
        "  #   # X_train, X_test, y_train, y_test\n",
        "  #   non_used_X_train, used_X_train, non_used_y_train, used_y_train = train_test_split(X, y, stratify=y, test_size=0.45) # 0.08\n",
        "  #   mydf_X_orig = used_X_train\n",
        "  # else:\n",
        "  #   mydf_X_orig = mydf_X_orig.loc[:(data_count-1)]\n",
        "  #   mydf_X_orig = mydf_X_orig.drop('label', axis=1)\n",
        "\n",
        "  mydf_X_orig = mydf_X_orig.drop('label', axis=1)\n",
        "  \n",
        "  mydf_X_orig = mydf_X_orig.reset_index()\n",
        "  mydf_X_orig = mydf_X_orig.drop(\"index\", axis = 1)\n",
        "\n",
        "  arr_X_orig = get_tensor_array(mydf_X_orig)\n",
        "  arr_Y_orig = np.zeros(len(arr_X_orig))\n",
        "\n",
        "  # print(\"arr_X_orig.shape: \" + str(arr_X_orig.shape))\n",
        "  # print(\"arr_Y_orig.shape: \" + str(arr_Y_orig.shape))\n",
        "\n",
        "  mydf_X_rot_list = []\n",
        "\n",
        "  for i in range(len(mydf_X_orig)):\n",
        "    image = mydf_X_orig[img_str][i]\n",
        "    rot_img = image.rotate(angle=180)\n",
        "    mydf_X_rot_list.append(rot_img)\n",
        "\n",
        "  # print(len(mydf_X_rot_list))\n",
        "\n",
        "  mydf_X_rot = pd.DataFrame(mydf_X_rot_list, columns=[img_str])\n",
        "  # print(\"mydf_X_rot.shape: \" + str(mydf_X_rot.shape))\n",
        "\n",
        "  arr_X_rot = get_tensor_array(mydf_X_rot)\n",
        "  arr_Y_rot = np.ones(len(arr_X_rot))\n",
        "\n",
        "  # print(\"arr_X_rot.shape: \" + str(arr_X_rot.shape))\n",
        "  # print(\"arr_Y_rot.shape: \" + str(arr_Y_rot.shape))\n",
        "\n",
        "  return arr_X_orig, arr_Y_orig, arr_X_rot, arr_Y_rot\n",
        "\n",
        "def do_shuffling(my_arr_X, my_arr_y):\n",
        "  index_array = np.arange(my_arr_X.shape[0])\n",
        "  sample_index = np.random.choice(index_array, my_arr_X.shape[0], replace=False)\n",
        "\n",
        "  my_arr_X = my_arr_X[sample_index]\n",
        "  my_arr_y = my_arr_y[sample_index]\n",
        "\n",
        "  return my_arr_X, my_arr_y"
      ],
      "metadata": {
        "id": "MGrvyGaGrMqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_X_orig = pd.DataFrame(dataset['train'])\n",
        "test_df_X_orig = pd.DataFrame(dataset['test'])\n",
        "\n",
        "# deleting rows with symmetric numbers from the dataset \n",
        "delete_row = [0, 1, 3, 6, 8, 9]\n",
        "\n",
        "for i in range(len(delete_row)):\n",
        "  train_df_X_orig = train_df_X_orig[train_df_X_orig.label != delete_row[i]]\n",
        "  test_df_X_orig = test_df_X_orig[test_df_X_orig.label != delete_row[i]]\n",
        "\n",
        "# to keep the track of the indices of the test dataset; later we will need it to find out which image is classified incorrectly\n",
        "test_index_list = list(test_df_X_orig.index)\n",
        "\n",
        "train_df_X_orig = train_df_X_orig.reset_index()\n",
        "test_df_X_orig = test_df_X_orig.reset_index()\n",
        "\n",
        "train_df_X_orig = train_df_X_orig.drop(\"index\", axis = 1)\n",
        "test_df_X_orig = test_df_X_orig.drop(\"index\", axis = 1)\n",
        "\n",
        "print(\"before preprocessing:\")\n",
        "print(\"train_df_X_orig.shape: \" + str(train_df_X_orig.shape))\n",
        "print(\"test_df_X_orig.shape: \" + str(test_df_X_orig.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijYdvSU_4xOz",
        "outputId": "475f317a-cd70-442d-d395-876bc54bb682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before preprocessing:\n",
            "train_df_X_orig.shape: (23486, 2)\n",
            "test_df_X_orig.shape: (3934, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make train set\n",
        "train_arr_X_orig, train_arr_y_orig, train_arr_X_rot, train_arr_y_rot = do_preprocessing(train_df_X_orig, 1000, 1)\n",
        "\n",
        "train_X = np.concatenate((train_arr_X_orig, train_arr_X_rot))\n",
        "train_y = np.concatenate((train_arr_y_orig, train_arr_y_rot))\n",
        "\n",
        "train_X, train_y = do_shuffling(train_X, train_y)\n",
        "\n",
        "# make test set\n",
        "test_arr_X_orig, test_arr_y_orig, test_arr_X_rot, test_arr_y_rot = do_preprocessing(test_df_X_orig, 500, 0)\n",
        "\n",
        "test_X = np.concatenate((test_arr_X_orig, test_arr_X_rot))\n",
        "test_y = np.concatenate((test_arr_y_orig, test_arr_y_rot))\n",
        "\n",
        "print(\"after preprocessing:\")\n",
        "print(\"train_X.shape: \" + str(train_X.shape))\n",
        "print(\"train_Y.shape: \" + str(train_y.shape))\n",
        "print(\"test_X.shape: \" + str(test_X.shape))\n",
        "print(\"test_Y.shape: \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi8KGlUksxG1",
        "outputId": "6126249f-e1c8-417d-f74b-e989369596e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after preprocessing:\n",
            "train_X.shape: (46972, 32, 32, 1)\n",
            "train_Y.shape: (46972,)\n",
            "test_X.shape: (7868, 32, 32, 1)\n",
            "test_Y.shape: (7868,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_X.astype(\"float32\") / 255\n",
        "test_X = test_X.astype(\"float32\") / 255\n",
        "\n",
        "train_y = np.asarray(train_y).astype('float32').reshape((-1,1))\n",
        "test_y = np.asarray(test_y).astype('float32').reshape((-1,1))\n",
        "\n",
        "print(\"train_X.shape: \" + str(train_X.shape))\n",
        "print(\"train_Y.shape: \" + str(train_y.shape))\n",
        "print(\"test_X.shape: \" + str(test_X.shape))\n",
        "print(\"test_Y.shape: \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNBKzlRVtgnL",
        "outputId": "8669c99b-2593-4715-adfb-0a29215d42a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X.shape: (46972, 32, 32, 1)\n",
            "train_Y.shape: (46972, 1)\n",
            "test_X.shape: (7868, 32, 32, 1)\n",
            "test_Y.shape: (7868, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build a neural network (using Tensorflow, PyTorch, or any framework you like)"
      ],
      "metadata": {
        "id": "qC0v0I6D-w6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "height = 32\n",
        "width = 32\n",
        "channels = channel_count\n",
        "\n",
        "# n_classes = 360\n",
        "input_shape = (height, width, channels)\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "JeWX_OcP-1mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping,CSVLogger,ReduceLROnPlateau,TensorBoard\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "J_KFmUE9uW_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = tf.keras.applications.VGG19(\n",
        "    include_top=False,\n",
        "    # weights=\"imagenet\",\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    input_shape=input_shape,\n",
        "    pooling=None,\n",
        ")\n",
        "\n",
        "base_model = vgg_model\n",
        "\n",
        "import tensorflow.keras as K\n",
        "model = K.models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(256, activation='relu'))\n",
        "model.add(K.layers.Dropout(0.2))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(128, activation='relu'))\n",
        "model.add(K.layers.Dropout(0.2))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(64, activation='relu'))\n",
        "model.add(K.layers.Dropout(0.2))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "# model.add(K.layers.Dense(1, activation='sigmoid', name='fc1'))\n",
        "model.add(K.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"added\")\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=K.optimizers.Adam(learning_rate=0.001),\n",
        "                  metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "print(\"compiled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS_7DvYFuZdx",
        "outputId": "c3ba049b-4243-423b-c330-dbc97d4a6e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added\n",
            "compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train it to classify image orientation until a reasonable accuracy is reached"
      ],
      "metadata": {
        "id": "yfPA0DBhvK8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5)\n",
        "\n",
        "batch_size = 64\n",
        "nb_epoch = 50\n",
        "history = model.fit(train_X, train_y, \n",
        "                    batch_size=batch_size, epochs=nb_epoch,\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=1, \n",
        "                    validation_split=0.2, \n",
        "                    # validation_data = (valid_X, valid_y),\n",
        "                    shuffle=True\n",
        "                    )\n",
        "\n",
        "print(\"fitted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUG7inAtulCx",
        "outputId": "4ea8eb60-b7fa-4bf6-c249-a8dff99bd218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "588/588 [==============================] - 79s 110ms/step - loss: 0.1407 - accuracy: 0.9319 - val_loss: 0.6540 - val_accuracy: 0.7750\n",
            "Epoch 2/50\n",
            "588/588 [==============================] - 63s 107ms/step - loss: 0.0402 - accuracy: 0.9856 - val_loss: 0.1781 - val_accuracy: 0.9256\n",
            "Epoch 3/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0293 - accuracy: 0.9901 - val_loss: 2.3166 - val_accuracy: 0.5069\n",
            "Epoch 4/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0225 - accuracy: 0.9926 - val_loss: 0.1270 - val_accuracy: 0.9613\n",
            "Epoch 5/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.3757 - val_accuracy: 0.8899\n",
            "Epoch 6/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0196 - accuracy: 0.9938 - val_loss: 3.2893 - val_accuracy: 0.5355\n",
            "Epoch 7/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 3.3238 - val_accuracy: 0.4941\n",
            "Epoch 8/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.8019 - val_accuracy: 0.8123\n",
            "Epoch 9/50\n",
            "588/588 [==============================] - 64s 108ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.6903 - val_accuracy: 0.7947\n",
            "Epoch 9: early stopping\n",
            "fitted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEJGcig23X1z",
        "outputId": "0ce98654-b78a-49a6-f381-02f8498c480c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_history = history\n",
        "plt.figure(figsize=(18,8))\n",
        "\n",
        "plt.suptitle('Loss and Accuracy Plots', fontsize=18)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(model_history.history['loss'], label='Training Loss')\n",
        "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of epochs', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(model_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of epochs', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZfJSFqOQ3Lsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Look at some of the images that were classified incorrectly. Please explain what you might do to improve your model's performance on these images in the future (you do not need to impelement these suggestions)"
      ],
      "metadata": {
        "id": "60jdL9Dyc0Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_labels = model.predict(test_X)\n",
        "y_pred = (model.predict(test_X) > 0.5).astype(\"int32\") "
      ],
      "metadata": {
        "id": "DdgA7aGBc7BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count_0 = 0\n",
        "# count_1 = 0\n",
        "\n",
        "# print(len(test_y)/2)\n",
        "# for i in range(len(test_y)):\n",
        "#     if test_y[i] == y_pred[i]:\n",
        "#         if test_y[i] == 0:\n",
        "#             count_0 += 1\n",
        "#         else:\n",
        "#             count_1 += 1\n",
        "            \n",
        "# print(count_0)\n",
        "# print(count_1)"
      ],
      "metadata": {
        "id": "CVF_kt1rwyxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "labels = ['0', '1']\n",
        "print(classification_report(test_y, y_pred, target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GksFmp2Seq2Z",
        "outputId": "49be0394-5f5e-432a-a496-4206b7bfbfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      3934\n",
            "           1       1.00      0.58      0.73      3934\n",
            "\n",
            "    accuracy                           0.79      7868\n",
            "   macro avg       0.85      0.79      0.78      7868\n",
            "weighted avg       0.85      0.79      0.78      7868\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### correctly classified image"
      ],
      "metadata": {
        "id": "Gmhq-yFGmN2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_i = -1\n",
        "for i in range(len(test_y)):\n",
        "  if test_y[i] == y_pred[i]:\n",
        "    correct_i = i\n",
        "    break\n",
        "\n",
        "print(correct_i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DIsMdPOlxjS",
        "outputId": "fa227d71-b305-42c3-8427-f014fed7e403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_image_index = test_index_list[correct_i]\n",
        "check_image =  dataset['test'][check_image_index]['image']\n",
        "check_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "t9l3RdbimBRS",
        "outputId": "193ebe6d-f105-4f99-9b86-89e9c588be59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F17302BF490>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### incorrectly classified image"
      ],
      "metadata": {
        "id": "CA1LI7v6mV3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_i = -1\n",
        "\n",
        " # making a list of incorrectly classified images\n",
        "\n",
        "incorrect_list = []\n",
        "for i in range(len(test_y)):\n",
        "  if test_y[i] != y_pred[i]:\n",
        "    incorrect_list.append(i)\n",
        "    # incorrect_i = i\n",
        "    # break\n",
        "\n",
        "# print(incorrect_i)"
      ],
      "metadata": {
        "id": "eqTP4W7xewug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# incorrectly classified image 1\n",
        "\n",
        "half_len_test = int(len(test_y)/2)\n",
        "\n",
        "incorrect_i = incorrect_list[0]\n",
        "if incorrect_i >= half_len_test:\n",
        "  incorrect_i = incorrect_i - half_len_test\n",
        "check_image_index = test_index_list[incorrect_i]\n",
        "check_image =  dataset['test'][check_image_index]['image']\n",
        "check_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "3UViHS_pfBAg",
        "outputId": "5fd70eb7-e860-4f0a-eb0c-d861b5947406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F16BB4EBC50>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rot_check_img = check_image.rotate(angle=180)\n",
        "rot_check_img\n",
        "\n",
        "# similarity with 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "U_l7KdL7iW2l",
        "outputId": "f0d29ee3-bdf2-47d4-fc91-690578947219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F16BB423190>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4ElEQVR4nGNgGGSAEUprBjEoJDEwrLzKwPC7CyFpacNg4sfAxILQsDEIzqz+CwHL+ldAWT8QOrk6mRkY+t8zfPzNys/AwCB+ieE3By5HSCLpRAc8a//+bcMl2fv37wlhXBr3/v3hh0fjMVxyyi//vpLEZej8v9824NI44e/fBlxy9s//fvfBIcd17O/3UlwaHf/+vcjAwMAgJA8TYUKW/r2SgYFB8sAdX0ydPX/PsTHwKFz6+8sEKoIUje8Y9Ffdc9dgYFh/BlOn3C1IdK6EizAiycZaZjKsufS9B5eLqQUAZ6hZusjlQKsAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# incorrectly classified image 2\n",
        "\n",
        "half_len_test = int(len(test_y)/2)\n",
        "incorrect_i = incorrect_list[1]\n",
        "if incorrect_i >= half_len_test:\n",
        "  incorrect_i = incorrect_i - half_len_test\n",
        "check_image_index = test_index_list[incorrect_i]\n",
        "check_image =  dataset['test'][check_image_index]['image']\n",
        "check_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "cdGfLEOz18E6",
        "outputId": "ac8aecc4-b12e-4c57-d0c2-8cb94264d952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F16BB423850>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgGLRA7ECbAozN78uKIif4+tdKuNydj6rIciJ7/06Gc7r/pqBodPv7VxTG1v63lhfFwpl/E+Byz//FoGhc/P8MN4yd8W8eqlMX/d0EdR9n89u/MFEWGMN714fpDAwM9g4WDGtQNTIYP/n799/fv3///vv797Yyms6zugYepa8XMjAsvshw7C4DDqD075woLjmGBX9dccqF/vtohFNy3r+lOOUYnn/BrTHj3wvcGi/8ncvAKwfnMqFJ/43e34xT57+/s2RxSNruaxBnw20rlQAAKNJLfTqR0FsAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rot_check_img = check_image.rotate(angle=180)\n",
        "rot_check_img\n",
        "\n",
        "# similarity with 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "aebC4iU-2BTO",
        "outputId": "16ca4e7f-270f-459b-9bf7-0f8064ade547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F16BB3DCCD0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgoDVgE2/YZ4tDTnbW339/L6AIMcFZLUZxc3CaKsfLMBdNJwp48S8Dp5zRl+e4NS79Nw+3xo//QnFKuv5dgFNO9Nw/JTQhFjirR/8/VycDQ7xY944L79FUKd/++/ff379///77+/eJMUSMESa5MoThxIGDDAwMmQJ2DEvjUHX+fdvMCWGxbvq7CFVuHiJwuM/8X4wiF/PvuTaMnfB3phiyHO/af3A50b9/3VA0pvzthrMn/90rgiyn+vEOP9zVv14Lomhk9YXLKbQdQLFwcAEAMrdK2wFReRAAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My train accuracy is 0.9951 and test accuracy is 0.79.\n",
        "The validation curve is not smooth, hence there may be some overfitting problem. To improve the model, hypermeter tuning as well as cross validation can be used. Also curating some data by observing may also help to improve model learning."
      ],
      "metadata": {
        "id": "G0kTF7MBivX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write up**: \n",
        "* Link to the model on Hugging Face Hub: \n",
        "* Include some examples of misclassified images. Please explain what you might do to improve your model's performance on these images in the future (you do not need to impelement these suggestions)"
      ],
      "metadata": {
        "id": "qSeLed2JxvGI"
      }
    }
  ]
}